[{"content":"Welcome to my little notebook 🍄 Current reseach focus: Computer Vision, SLAM Cool things I\u0026rsquo;d like to do: LLM Agents, RL ","date":"2025-02-20T00:00:00Z","image":"https://gushroom.github.io/p/hello-world/img_hu_9a39d11d44995c65.jpeg","permalink":"https://gushroom.github.io/p/hello-world/","title":"Hello World"},{"content":" 1 2 3 4 5 6 /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [37,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [38,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [39,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [40,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [41,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [42,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. Try catch doesn\u0026rsquo;t work here. If anything we should try to assert before crashing out like that\nMoving to CPU will help debugging.\n","date":"2025-03-24T00:00:00Z","permalink":"https://gushroom.github.io/p/cuda-assertion-fail/","title":"Cuda Assertion Fail"},{"content":"Policy Gradient 强化学习的核心之一就是学以致用。一边在探索环境中学习，一边用学到的action来获取reward。Policy是环境到行为的映射，policy(state) = action, 同理还有state(action) = reward。\nAt every step, the agent takes an action based on the policy. If the action space is discrete, the policy should return a softmax probability of each action, and if the action space is continuous, the policy outputs the mean and standard deviation of the Gaussian probability distribution for each continuous action.\n为什么连续的空间要输出高斯分布呢？\n均值（$\\mu$）：代表智能体认为在当前状态下最优的动作值。 标准差（$\\theta$）：控制探索的随机性（$\\theta$ 越大，动作的随机性越强）。 Policy 既然是一个映射（函数），那我们就可以用神经网络来拟合\nCartPole的环境比较简单，用一个小MLP就可以了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch import torch.nn as nn import torch.nn.functional as F from torch import distributions class PolicyNetwork(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim, dropout): super().__init__() self.layer1 = nn.Linear(input_dim, hidden_dim) self.layer2 = nn.Linear(hidden_dim, output_dim) self.dropout = nn.Dropout(dropout) self.relu = nn.ReLU() def forward(self, x): x = self.layer1(x) x = self.dropout(x) x = self.relu(x) x = self.layer2(x) return x 每一步计算长期return\nReturn是从这一步加后续所有奖励折扣之和\ndiscount_factor一般小于1，越远的回报权重越小\n在最前面插入，确保顺序是[R0, R1, R2...]\n1 2 3 4 5 6 7 8 9 def calculate_stepwise_returns(rewards, discount_factor): returns = [] R = 0 for r in reversed(rewards): # 先 R = r + R * discount_factor returns.insert(0, R) returns = torch.tensor(returns) normalized_returns = (returns - returns.mean()) / returns.std() return normalized_returns 为什么要Normalize?\n减小方差 加速收敛 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def forward_pass(env, policy, discount_factor): log_prob_actions = [] rewards = [] done = False episode_return = 0 policy.train() observation, info = env.reset() while not done: observation = torch.FloatTensor(observation).unsqueeze(0) # state action_pred = policy(observation) # action = policy(state) action_prob = F.softmax(action_pred, dim = -1) dist = distributions.Categorical(action_prob) # discretize action = dist.sample() log_prob_action = dist.log_prob(action) observation, reward, terminated, truncated, info = env.step(action.item()) # reward = state(action) done = terminated or truncated log_prob_actions.append(log_prob_action) rewards.append(reward) episode_return += reward log_prob_actions = torch.cat(log_prob_actions) stepwise_returns = calculate_stepwise_returns(rewards, discount_factor) return episode_return, stepwise_returns, log_prob_actions 计算loss\n1 2 3 def calculate_loss(stepwise_returns, log_prob_actions): loss = -(stepwise_returns * log_prob_actions).sum() return loss 这里的loss function就是最基本的policy gradient, REINFORCE\n$$ \\tt{Loss} = -\\sum_t (\\tt{Return}_t \\times \\log \\pi (a_t | s_t))$$$\\log \\pi (a_t | s_t)$ 是策略网络输出的动作对数概率（log_prob_actions）\n并反向传播更新权重\n1 2 3 4 5 6 7 8 def update_policy(stepwise_returns, log_prob_actions, optimizer): stepwise_returns = stepwise_returns.detach() loss = calculate_loss(stepwise_returns, log_prob_actions) optimizer.zero_grad() loss.backward() optimizer.step() return loss.item() 训练并保存权重\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import gymnasium as gym import torch import numpy as np from torch import optim env = gym.make(\u0026#34;CartPole-v1\u0026#34;) seed = 66 np.random.seed(seed) torch.manual_seed(seed) env.reset(seed=seed) def main(): MAX_EPOCHS = 500 DISCOUNT_FACTOR = 0.99 N_TRIALS = 25 REWARD_THRESHOLD = 475 PRINT_INTERVAL = 10 INPUT_DIM = env.observation_space.shape[0] HIDDEN_DIM = 128 OUTPUT_DIM = env.action_space.n DROPOUT = 0.5 episode_returns = [] policy = PolicyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT) LEARNING_RATE = 0.01 optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE) for episode in range(1, MAX_EPOCHS+1): episode_return, stepwise_returns, log_prob_actions = forward_pass(env, policy, DISCOUNT_FACTOR) _ = update_policy(stepwise_returns, log_prob_actions, optimizer) episode_returns.append(episode_return) mean_episode_return = np.mean(episode_returns[-N_TRIALS:]) if episode % PRINT_INTERVAL == 0: print(f\u0026#39;| Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} |\u0026#39;) if mean_episode_return \u0026gt;= REWARD_THRESHOLD: print(f\u0026#39;Reached reward threshold in {episode} episodes\u0026#39;) torch.save(policy, \u0026#39;pg.pt\u0026#39;) break if __name__ == \u0026#34;__main__\u0026#34;: main() Load model and let it play\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import gymnasium as gym import torch env = gym.make(\u0026#34;CartPole-v1\u0026#34;, render_mode=\u0026#34;human\u0026#34;) observation, info = env.reset() policy = torch.load(\u0026#34;ppo.pt\u0026#34;, weights_only=False).eval() episode_over = False while not episode_over: obs_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0) # Get action from policy with torch.no_grad(): action_logits = policy(obs_tensor) action = torch.argmax(action_logits, dim=1).item() observation, reward, terminated, truncated, info = env.step(action) episode_over = terminated or truncated env.close() ","date":"2025-03-02T00:00:00Z","permalink":"https://gushroom.github.io/p/cartpole/","title":"Cartpole"},{"content":"the CLIP/SigLip model family has 3 parameters. VIT-Patch-Res. VIT is the vision transformer architecture and size. Patch is when they split the input image into smaller grids, what is the size of those smaller images, 16 means 16x16, 14 means 14x14. Res is the input image resolution. The higher the more accurate but bigger(slower).\nPrereq: install git-lfs https://github.com/git-lfs/git-lfs/blob/main/INSTALLING.md\nDownload model weight: git clone https:/huggingface.co/\u0026lt;repository\u0026gt;\nRunning the model using transformers 1 2 3 4 5 6 7 8 9 10 11 from transformers import AutoModel, AutoProcessor, AutoTokenizer # load the model and processor ckpt = \u0026#34;path/to/model/directory\u0026#34; model = AutoModel.from_pretrained(ckpt).to(device).eval() processor = AutoProcessor.from_pretrained(ckpt) tokenizer = AutoTokenizer.from_pretrained(ckpt) inputs = processor(images=[image], return_tensors=\u0026#34;pt\u0026#34;).to(model.device) with torch.no_grad(): image_embeddings = model.get_image_features(**inputs) Running Qwen-VL2.5 1 2 model = Qwen2_5_VLForConditionalGeneration.from_pretrained(ckpt, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;) processor = AutoProcessor.from_pretrained(ckpt) What\u0026rsquo;s not mentioned in the documentation, PIL.Image is also accepted as image input\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def describe_images(self, image: PIL.Image, prompt=\u0026#34;Describe this image\u0026#34;) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34;For large VLM that output natural language instead of vector embeddings\u0026#34;\u0026#34;\u0026#34; from qwen_vl_utils import process_vision_info if self._model is None: self.load_model() messages = [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;image\u0026#34;, \u0026#34;image\u0026#34;: image, }, {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: prompt}, ], } ] text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) image_inputs, video_inputs = process_vision_info(messages) inputs = self.processor( text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\u0026#34;pt\u0026#34;, ) image_grid_thw = inputs[\u0026#39;image_grid_thw\u0026#39;].numpy() input_h = image_grid_thw[0][1] * 14 input_w = image_grid_thw[0][2] * 14 inputs = inputs.to(self.device) generated_ids = self._model.generate(**inputs, max_new_tokens=128) generated_ids_trimmed = [ out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids) ] output_text = self.processor.batch_decode( generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False ) return output_text[0], (input_w, input_h) ","date":"2025-03-02T00:00:00Z","permalink":"https://gushroom.github.io/p/download-and-run-vlm-model-from-huggingface/","title":"Download and run VLM model from HuggingFace"},{"content":"Omniparser(https://github.com/microsoft/OmniParser) provides grounding for GUI elements. This allows for fully automation and controll over devices in an agentic way.\nclone the repo, and prepare the environment:\n1 2 3 4 cd OmniParser conda create -n \u0026#34;omni\u0026#34; python==3.12 conda activate omni pip install -r requirements.txt Model weights are here:\n1 2 git clone https://huggingface.co/microsoft/OmniParser git clone https://huggingface.co/microsoft/OmniParser-v2.0 BLIP and Florence models are in the Omniparser repository, while v-2.0 has a newer YOLO detection model.\nPut model weights under OmniParser/weights/\nAutoDL 的系统盘只有30G，所以把模型权重放数据盘：\n1 2 3 4 5 6 终端中执行： export HF_HOME=/root/autodl-tmp/cache/ 或者Python代码中执行： import os os.environ[\u0026#39;HF_HOME\u0026#39;] = \u0026#39;/root/autodl-tmp/cache/\u0026#39; OCR的语言设置在util/utils.py\n1 2 3 4 5 6 7 8 9 10 reader = easyocr.Reader([\u0026#39;en\u0026#39;]) paddle_ocr = PaddleOCR( lang=\u0026#39;en\u0026#39;, # other lang also available use_angle_cls=False, use_gpu=False, # using cuda will conflict with pytorch in the same process show_log=False, max_batch_size=1024, use_dilation=True, # improves accuracy det_db_score_mode=\u0026#39;slow\u0026#39;, # improves accuracy rec_batch_num=1024) paddle的话，把en换成ch就是中英双语的了 easy可以设置成[\u0026lsquo;ch_sim\u0026rsquo;,\u0026rsquo;en\u0026rsquo;]\n最后把端口设置转发\n1 ssh -CNg -L 6006:127.0.0.1:6006 root@connect.nmb1.seetacloud.com -p 33674 在本地就可以运行了 ","date":"2025-03-01T00:00:00Z","permalink":"https://gushroom.github.io/p/running-omniparser-with-autodl-backend/","title":"Running Omniparser with AutoDL Backend"},{"content":"It all started with: Deep neural networks are universial function approximators.\nWhat is a unversial function approximator? A feed-forward artificial neural network with a single hidden layer and sigmoidal activation functions can pointwise approximate any continuous function of many variables with any predetermined accuracy.\nSuppose we have a set of points $(x, y)$ and assume that there is some regularity connecting $x$ and $y$:\n$$y=f(x)$$We can select and apply to $x$ such a weighting factor $w_{1}$ and such an offset $b_1$, that the sigmoid taken from them will pass through a part of our points or close enough to them:\n$$\\sigma_1=\\sigma(w_{1}x+b_1)$$We can then select and apply to $x $ such $w_{2}$ and $b_2$, that the sigmoid taken from them in sum with $\\sigma_ 1$ will pass still through a part of the points or close enough to them:\n$$\\sigma_2=\\sigma(w_{2}x+b_2)$$We can keep adding sigmoids and tinkering with their parameters until their sum approximates the pattern $f(x)$ expressed in the data accurately enough.\n$$\\sigma_1+\\sigma_2+...+\\sigma_n \\approx f(x)$$How do they become universial function approximators? As we all know, with just $y = (wx+b)$, we have linear regression. We know linear regressions are weak, they cannot solve XOR classification, as an example. What does the trick, is activation function. Activation functions introduce non-linearity to the model, allowing the model to fit to arbitrary functions.\nWhy we chose sigmoid()? Sigmoid() $\\sigma(x) = 1 - sigma(-x)$ function (0, 1) simulates the probability of \u0026ldquo;firing a neuron\u0026rdquo;, and it has been widely used in machine learning and statistics. It was one of the easiest functions to add non-linearity as well.\nFrom sigmoid() to tanh() Sigmoid() function only has positive values, it passes the same sign to update the weight matrices during back propagation, leading to inefficient gradient descent.\nTanh(), hyperbolic tangent, solves the above problem by centering at 0, so we can have both signs when updating weight. Converges faster.\nReLU and its brothers Vanishing gradient problem: As models get deeper, the gradient of the loss $\\mathcal{L}$ with respect to the weights in layer $l$ can be written as:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}_\\mathcal{L}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_L} \\times \\frac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{a}_{L-1}} \\times ... \\times \\frac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}}$$where $\\mathbf{a}_l$ is the activation at layer $l$.\nWe can see that when $\\partial \\mathbf{a}_l$ is less than 1, (especially sigmoid, the max is only 0.25), the result approaches 0 exponentially. Our model stops learning from here.\nReLU is a easy stepwise funtion that mitigates this problem. Its derivative is always 1 when input is positive, and it does not saturate.\nHowever, a rigid threshold of $y = 0(x \u0026lt;= 0)$ also causes problems.\nIt \u0026ldquo;turns off\u0026rdquo; neurons with nagative weight. Leaky ReLU It is not differentiable at $(x = 0)$ (Same holds for Leaky) GELU, Swish Normalization Optimizers Loss functions ","date":"2025-02-25T00:00:00Z","permalink":"https://gushroom.github.io/p/a-little-bit-on-deep-learning-in-general/","title":"A Little Bit on Deep Learning in General"},{"content":"CNN Convolution 卷积 = 特征提取\n索伯算子 https://en.wikipedia.org/wiki/Sobel_operator 既是传统的图像处理，也是很好的特征提取例子 为什么卷积的同时通道数也在增加？ 卷积和池化都会减少dimension，增加通道数来保证不丢失太多信息量 n_channels个卷积核随机初始化，对同一个feature map进行特征提取 每个通道可以学习不同的特征 1x1卷积核可以在不改变feature map大小的前提下改变通道数 升维/降维 通道间传递信息：使用1x1卷积核，实现降维和升维的操作其实就是channel间信息的线性组合变化，3x3，64channels的卷积核后面添加一个1x1，28channels的卷积核，就变成了3x3，28channels的卷积核，原来的64个channels就可以理解为跨通道线性组合变成了28channels，这就是通道间的信息交互 Key components of CNN: Convolution: Extract feature from images, value in each cell are randomized and learned. Pooling: Reduce dimensionality to prevent overfitting, reduce computational cost(reduce size of feature map). Activation: Introduce Non-linearity. Fully connected layers: Perform classification. Size calculation: Convolution: $\\tt{size} = \\lfloor \\frac{(\\tt{width - kernelsize} + 2 \\times \\tt{padding})}{\\tt{stride}} \\rfloor + 1$ Pooling: $\\tt{size} = \\lceil \\frac{size}{2} \\rceil$ When size remains the same: kernelsize = 3 and padding = 1, kernel = 5 and padding = 2 VIT (Vision Transformer) Trains faster than CNN (transformer is more suitable for parallel computation) Requires slightly more dataset to train (at smaller datasets lose to CNN, excels at larger datasets) Attempt to apply transformer achitecture on image tasks without change represent image features like text patch(224 = 14px * 14px * (4x4)grid) Flattern pixels in the patch Linear projection of flatterned patches Add a learnable position encoder, attach to image token vectors ","date":"2025-02-25T00:00:00Z","permalink":"https://gushroom.github.io/p/computer-vision/","title":"Computer Vision"},{"content":"Answering all of my own questions about what I know that I dont know.\nWhat is encoder and decoder? Encoding Encoding is the process of extracting semantic information from languge. For some representation to be a good choice of encoding used in machine learning, it must satisfy 2 standards:\nIt must be easily digitized(understood by computers) The relationship between encodings must somehow represent the relationship between original language tokens. Let\u0026rsquo;s focus on (1.) first. To that end, we have tokenizer and one-hot encoding. Tokenizer gives each token an unique id, thus projecting the entire token space onto a one-dimensional axis(array). One-hot encoding uses unique binary ids, projecting each token onto it\u0026rsquo;s own dimension, keeping distances between encoded tokens the same. However they both fail to achieve the second standard.\nTokenizer makes it simple to calculate relationships(difference) between encoded tokens, but fails to capture the complex relationships between tokens, especially when some words have multiple meanings in different contexts. One-hot encoding cannot represent the relationship between tokens with relative distance at all. Token vectors are all perpendicular to each other, and the dot product is always 0. However, it makes addition simple, we can have combination of any vectors. Latent space and embedding We need some intermediate space, uses advantages from both tokenizer that captures relationship but is too dense, and one-hot encoding that allows easy combination(addition) of vectors but is too sparse. We can either add more dimensions to the one-dimensional tokenizer, or reduce dimensions from one-hot encoded space. Embedding is the process of reducing dimensionality from one-hot encoding by neural networks to a lower dimensional latent space.\nTokenizer - Splitting text corpuses into language primitives BPE (Byte-Pair Encoding) A very detailed step by step explanation can be found here: https://huggingface.co/learn/nlp-course/en/chapter6/5\nSplit text corpuses into primitives, such as individual characters(ascii, unicode), then iteratively merge the most frequent combination of the current primitives into a new token. We merge until we have got a satisfactory vocabulary size.\nBBPE Ascii and unicode have problems with representing unknown chars, such as Chinese, emoji. BBPE splits corpuses into bytes, having better representation and support for mulitlingual and special characters.\nHow do we poject token into embedding? We need to find a relationship to properly embed tokens as vectors.\nWord2Vec In 2013, Google proposed Word2Vec model, the first low-dimension word embedding, introducing two breakthroughs: Continuous Bag of Words(CBOW) and skip-gram.\nCBOW: collects words that appear before and after a target word in a sequence. For example, \u0026ldquo;The cat sits on the mat.\u0026rdquo; and the target word is \u0026ldquo;sits\u0026rdquo;, the model learns by trying to predict \u0026ldquo;sits\u0026rdquo; from [\u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;the\u0026rdquo;]. The model learns a weight to transform words into embeddings that has the max probability to predict the right word. Skip-grams: the opposite, predicts n context words based on a given target word. What it does: King - man + women $\\approx$ queen. Vector carries semantic meanings and can be \u0026ldquo;calculated\u0026rdquo;. Cosine similarity can actually find similarity between semantically close vectors. Problems with Word2Vec: It only considers one meaning of each word. (i.e. bank) Most importantly, it does not consider long range context information. LSTM Better aptures long range context, but still has a lot of issues:\nLSTM process word one by one, this cannot be parallelized, making training slow. Still struggles with longer context length. WIP\u0026hellip;Not tooo interested in RNN and LSTM and GRU tho\u0026hellip;\nTransformers Self Attention $$\\tt{Attention}(\\mathbf{Q, K, V}) = \\tt{softmax} \\lparen \\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\rparen \\mathbf{V}$$ An interesting observation is $\\mathbf{QK}^T$ is matrix multiplication, but it is called \u0026ldquo;dot product\u0026rdquo; because it effectively computes row-wise similarity score, and dot product is usually used for that purpose.\nThis similarity computation \u0026ldquo;assigns more weight\u0026rdquo; to any previous token $K$ that has a closer relationship with $Q$, allows the model to focus on more relevant information.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class SelfAttention(nn.Module): def __init__(self, dim_in, dim_out, qkv_bias = False): super().__init__() self.query = nn.Linear(dim_in, dim_out, bias=qkv_bias) self.key = nn.Linear(dim_in, dim_out, bias=qkv_bias) self.value = nn.Linear(dim_in, dim_out, bias=qkv_bias) def forward(self, x): Q = self.query(x) K = self.key(x) print(K.shape) V = self.value(x) attn_scores = Q @ K.T attn_weights = torch.softmax( attn_scores / K.shape[-1] ** 0.5, dim=-1 ) context_vec = attn_weights @ V return context_vec A Linear() layer computes $y = xW^T + b$, it tranforms input vector $x$ into another space by weight matrix $W$ and bias $b$. In this case, it projects $x$ onto the embedding space.\nThe softmax() function computes $\\tt{softmax}(x_i) = \\frac{e^{x_i}}{\\sum^n_{j=1}e^{x_j}}$. It projects points on the real number axis $x_i$ onto the function $y_i = e^{x_i}$. Then it considers the sum of all $y$ values to be 1. Now we have a non-zero probability score.\nAnother interesting fact about the softmax function is: if we multiply every $x$ with a factor, the relationship between $y$ will change. When this factor is less than 1, we will see the probability distribution moving towards a uniform distribution. This is the temperature parameter. The lower the temperature, the more \u0026ldquo;random\u0026rdquo; the results are.\nWhat\u0026rsquo;s wrong with self attention? It only has one QKV set. The model only learns one set of weight matrix, limiting its ability to capture more features of the data.\nMultihead Attention Proposed in the Attention is All You Need paper in 2017 with the transformers achitecture.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class MultiheadAttention(nn.Module): def __init__(self, embedding_size, num_heads): super().__init__() assert embedding_size % num_heads == 0, \u0026#34;Embedding size must be divisible by num_heads\u0026#34; self.embedding_size = embedding_size self.num_heads = num_heads self.head_dim = embedding_size // num_heads self.query = nn.Linear(embedding_size, embedding_size) self.key = nn.Linear(embedding_size, embedding_size) self.value = nn.Linear(embedding_size, embedding_size) self.out = nn.Linear(embedding_size, embedding_size) def forward(self, x): batch_size, sequence_length, embedding_size = x.shape Q = self.query(x) # shape: (batch_size, sequence_length, embedding_size) K = self.key(x) V = self.value(x) Q = Q.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) K = K.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) V = V.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) # Q,K,V.shape(batch_size, self.num_heads, sequence_length, self.head_dim) # Why view + transpose instead of view(batch_size, sequence_length, self.num_heads, self.head_dim)? # This is because how view() split tensor dimensions, we want to split embedding_size to num_heads * head_dim # But in the end we want (batch_size, self.num_heads, sequence_length, self.head_dim) for future calculations scores = torch.matmul(Q, K.transpose(-2, -1)) # K.T.shape: (batch_size, self.num_heads, self.head_dim, sequence_length) # Q @ K.T shape: (batch_size, self.num_heads, sequence_length, sequence_length) # (sequence_length, sequence_length) representing the attention scores for each head across all sequence positions normalized_scores = scores / self.head_dim ** 0.5 attention_weights = nn.functional.softmax(normalized_scores, dim = -1) attention_outputs = torch.matmul(attention_weights, V) # V.shape: (batch_size, self.num_heads, sequence_length, self.head_dim) # attention_output shape: (batch_size, self.num_heads, sequence_length, head_dim) combined_heads = attention_outputs.transpose(1, 2).contiguous().view(batch_size, sequence_length, embedding_size) # attention_outputs.transpose(1, 2): (batch_size, sequence_length, self.num_heads, head_dim) output = self.out(combined_heads) return output Note that besides the split and join of multiple attention heads, in the final layer we project attention output into embedding space.\nWhat\u0026rsquo;s wrong with MHA? MHA is great in terms of quality, the problem is computation cost. In MHA we repeat the attention calculation num_heads times, and that starts to become a problem when the model size get bigger. In order to compute attention_outputs, we need to load the QKV matricies into memory many times. and data transferring also becomes a bottleneck.\nSo we reduce amount of heads.\nMulti Query Attention Here we keep only one K and V, and we split Q into num_heads.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class MultiQueryAttention(nn.Module): def __init__(self, embedding_size, num_heads): super().__init__() assert embedding_size % num_heads == 0, \u0026#34;Embedding size must be divisible by num_heads\u0026#34; self.embedding_size = embedding_size self.num_heads = num_heads self.head_dim = embedding_size // num_heads # Separate query projection for multiple heads self.query = nn.Linear(embedding_size, embedding_size) # Shared key and value projection (single head) self.key = nn.Linear(embedding_size, self.head_dim) self.value = nn.Linear(embedding_size, self.head_dim) self.out = nn.Linear(embedding_size, embedding_size) def forward(self, x): batch_size, sequence_length, embedding_size = x.shape Q = self.query(x) # (batch_size, sequence_length, embedding_size) K = self.key(x) # (batch_size, sequence_length, head_dim) V = self.value(x) # (batch_size, sequence_length, head_dim) # Reshape queries to multiple heads Q = Q.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) # Keys and values are shared across all heads, so they have a single head dimension K = K.unsqueeze(1).expand(-1, self.num_heads, -1, -1) V = V.unsqueeze(1).expand(-1, self.num_heads, -1, -1) # K.unsqueeze(1) = (batch_size, 1, sequence_length, head_dim) # K.expand = (batch_size, num_heads, sequence_length, head_dim) scores = torch.matmul(Q, K.transpose(-2, -1)) / self.head_dim ** 0.5 # Q @ K.T = (batch_size, num_heads, sequence_length, sequence_length) attention_weights = nn.functional.softmax(scores, dim=-1) attention_outputs = torch.matmul(attention_weights, V) combined_heads = attention_outputs.transpose(1, 2).contiguous().view(batch_size, sequence_length, embedding_size) output = self.out(combined_heads) return output Group Query Attention Between MLA and MQA, uses G(1 \u0026laquo; G \u0026laquo; num_heads) Groups of KV pair, and still num_heads amount of Q matrices.\nWhen G = 1, GQA = MQA, when G = num_heads, GQA = MHA.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class GroupQueryAttention(nn.Module): def __init__(self, embedding_size, num_heads, num_groups): super().__init__() assert embedding_size % num_heads == 0, \u0026#34;embedding_size must be divisible by num_heads!\u0026#34; assert embedding_size % num_groups == 0, \u0026#34;embedding_size must be divisible by num_groups!\u0026#34; assert num_heads % num_groups == 0, \u0026#34;num_heads must be divisible by num_groups!\u0026#34; self.embedding_size = embedding_size self.num_heads = num_heads self.num_groups = num_groups self.head_dim = embedding_size // num_heads self.heads_per_group = num_heads // num_groups self.query = nn.Linear(embedding_size, embedding_size) self.key = nn.Linear(embedding_size, embedding_size // self.heads_per_group) self.value = nn.Linear(embedding_size, embedding_size // self.heads_per_group) self.out = nn.Linear(embedding_size, embedding_size) def forward(self, x): batch_size, sequence_length, embedding_size = x.shape Q = self.query(x) # (batch_size, sequence_length, embedding_size) K = self.key(x) # (batch_size, sequence_length, embedding_size // heads_per_group) V = self.value(x) # (batch_size, sequence_length, embedding_size // heads_per_group) Q = Q.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) K = K.view(batch_size, sequence_length, self.num_groups, -1).transpose(1, 2) V = V.view(batch_size, sequence_length, self.num_groups, -1).transpose(1, 2) # (batch_size, num_groups, sequence_length, head_dim) K = K.unsqueeze(2).expand(-1, -1, self.heads_per_group, -1, -1) V = V.unsqueeze(2).expand(-1, -1, self.heads_per_group, -1, -1) # expand heads_per_group into the third dimension # (batch_size, num_groups, heads_per_group, sequence_length, head_dim) K = K.contiguous().view(batch_size, self.num_heads, sequence_length, self.head_dim) ","date":"2025-02-25T00:00:00Z","permalink":"https://gushroom.github.io/p/overview-of-nlpllm/","title":"Overview of NLP(LLM)"},{"content":"Building a Chatbot with Flet ChatGPT has becoming more and more difficult to use with VPN. So I decided to build one with API. Getting famailar with Flet(flutter) is an extra because I want to build more stuff on this platform.\nFlet The interaction logic is very easy: onclick=function(). With buttons:\n1 2 3 4 5 6 7 8 9 send_button = ft.ElevatedButton( text=\u0026#34;Send\u0026#34;, on_click=send_message, width=page.width * 0.2, style=ft.ButtonStyle( color=ft.Colors.WHITE, bgcolor=ft.Colors.BLUE_400, ) ) To enable markdown in response, we need to replace the TextField with a Markdown element.\n1 2 3 4 5 6 7 output_box = ft.Markdown( value=\u0026#34;\u0026#34;, selectable=True, extension_set=ft.MarkdownExtensionSet.GITHUB_WEB, on_tap_link=lambda e: page.launch_url(e.data), width=page.width * 0.7, ) Saving chat history and memory to local storage: page.client_storage.set(key, value)\nLLM A basic template to use LLM APIs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class ChatBot(): def __init__(self, api_key, base_url=None, system_prompt=None, model=\u0026#34;o3-mini-high\u0026#34;): default_system_prompt = \u0026#34;You are a helpful assistant.\u0026#34; default_base_url = \u0026#34;https://api.openai-next.com/v1\u0026#34; self.system_prompt = system_prompt if system_prompt else default_system_prompt self.messages=[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: self.system_prompt}] self.memories=[] self.model = model self.base_url = base_url if base_url else default_base_url self.client = OpenAI(api_key=api_key, base_url=self.base_url) def chat(self, user_message): self.messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_message}) completion = self.client.chat.completions.create( model=self.model, stream=False, messages=self.messages ) response = completion.choices[0].message.content self.messages.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: response}) return response ","date":"2025-02-20T00:00:00Z","permalink":"https://gushroom.github.io/p/building-a-chatbot-with-flet/","title":"Building a Chatbot with Flet"},{"content":"Hugo theme Stack supports the creation of interactive image galleries using Markdown. It\u0026rsquo;s powered by PhotoSwipe and its syntax was inspired by Typlog.\nTo use this feature, the image must be in the same directory as the Markdown file, as it uses Hugo\u0026rsquo;s page bundle feature to read the dimensions of the image. External images are not supported.\nSyntax 1 ![Image 1](1.jpg) ![Image 2](2.jpg) Result Photo by mymind and Luke Chesser on Unsplash\n","date":"2025-02-20T00:00:00Z","image":"https://gushroom.github.io/p/image-gallery/2_hu_3e58a979f20e4e46.jpg","permalink":"https://gushroom.github.io/p/image-gallery/","title":"Image gallery"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] One line code block 1 \u0026lt;p\u0026gt;A paragraph\u0026lt;/p\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nreference other pages More on math typesetting\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-02-20T00:00:00Z","permalink":"https://gushroom.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Stack has built-in support for math typesetting using KaTeX.\nIt\u0026rsquo;s not enabled by default side-wide, but you can enable it for individual posts by adding math: true to the front matter. Or you can enable it side-wide by adding math = true to the params.article section in config.toml.\nInline math This is an inline mathematical expression: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\n1 $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$ Block math $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ 1 2 3 $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ 1 2 3 $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ ","date":"2025-02-20T00:00:00Z","permalink":"https://gushroom.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"For more details, check out the documentation.\nBilibili video Tencent video YouTube video Generic video file Your browser doesn't support HTML5 video. Here is a link to the video instead. Gist GitLab Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n― A famous person, The book they wrote Photo by Codioful on Unsplash\n","date":"2025-02-20T00:00:00Z","image":"https://gushroom.github.io/p/shortcodes/cover_hu_5667347daefb4230.jpg","permalink":"https://gushroom.github.io/p/shortcodes/","title":"Shortcodes"}]