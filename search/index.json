[{"content":"Welcome to my little notebook ğŸ„ Current reseach focus: Computer Vision, SLAM Cool things I\u0026rsquo;d like to do: LLM Agents, RL ","date":"2025-02-20T00:00:00Z","image":"https://gushroom.github.io/p/hello-world/img_hu_9a39d11d44995c65.jpeg","permalink":"https://gushroom.github.io/p/hello-world/","title":"Hello World"},{"content":" 1 2 3 4 5 6 /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [37,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [38,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [39,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [40,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [41,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [25778,0,0], thread: [42,0,0] Assertion `-sizes[i] \u0026lt;= index \u0026amp;\u0026amp; index \u0026lt; sizes[i] \u0026amp;\u0026amp; \u0026#34;index out of bounds\u0026#34;` failed. Try catch doesn\u0026rsquo;t work here. If anything we should try to assert before crashing out like that\nMoving to CPU will help debugging.\n","date":"2025-03-24T00:00:00Z","permalink":"https://gushroom.github.io/p/cuda-assertion-fail/","title":"Cuda Assertion Fail"},{"content":"Policy Gradient å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒä¹‹ä¸€å°±æ˜¯å­¦ä»¥è‡´ç”¨ã€‚ä¸€è¾¹åœ¨æ¢ç´¢ç¯å¢ƒä¸­å­¦ä¹ ï¼Œä¸€è¾¹ç”¨å­¦åˆ°çš„actionæ¥è·å–rewardã€‚Policyæ˜¯ç¯å¢ƒåˆ°è¡Œä¸ºçš„æ˜ å°„ï¼Œpolicy(state) = action, åŒç†è¿˜æœ‰state(action) = rewardã€‚\nAt every step, the agent takes an action based on the policy. If the action space is discrete, the policy should return a softmax probability of each action, and if the action space is continuous, the policy outputs the mean and standard deviation of the Gaussian probability distribution for each continuous action.\nä¸ºä»€ä¹ˆè¿ç»­çš„ç©ºé—´è¦è¾“å‡ºé«˜æ–¯åˆ†å¸ƒå‘¢ï¼Ÿ\nå‡å€¼ï¼ˆ$\\mu$ï¼‰ï¼šä»£è¡¨æ™ºèƒ½ä½“è®¤ä¸ºåœ¨å½“å‰çŠ¶æ€ä¸‹æœ€ä¼˜çš„åŠ¨ä½œå€¼ã€‚ æ ‡å‡†å·®ï¼ˆ$\\theta$ï¼‰ï¼šæ§åˆ¶æ¢ç´¢çš„éšæœºæ€§ï¼ˆ$\\theta$ è¶Šå¤§ï¼ŒåŠ¨ä½œçš„éšæœºæ€§è¶Šå¼ºï¼‰ã€‚ Policy æ—¢ç„¶æ˜¯ä¸€ä¸ªæ˜ å°„ï¼ˆå‡½æ•°ï¼‰ï¼Œé‚£æˆ‘ä»¬å°±å¯ä»¥ç”¨ç¥ç»ç½‘ç»œæ¥æ‹Ÿåˆ\nCartPoleçš„ç¯å¢ƒæ¯”è¾ƒç®€å•ï¼Œç”¨ä¸€ä¸ªå°MLPå°±å¯ä»¥äº†\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch import torch.nn as nn import torch.nn.functional as F from torch import distributions class PolicyNetwork(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim, dropout): super().__init__() self.layer1 = nn.Linear(input_dim, hidden_dim) self.layer2 = nn.Linear(hidden_dim, output_dim) self.dropout = nn.Dropout(dropout) self.relu = nn.ReLU() def forward(self, x): x = self.layer1(x) x = self.dropout(x) x = self.relu(x) x = self.layer2(x) return x æ¯ä¸€æ­¥è®¡ç®—é•¿æœŸreturn\nReturnæ˜¯ä»è¿™ä¸€æ­¥åŠ åç»­æ‰€æœ‰å¥–åŠ±æŠ˜æ‰£ä¹‹å’Œ\ndiscount_factorä¸€èˆ¬å°äº1ï¼Œè¶Šè¿œçš„å›æŠ¥æƒé‡è¶Šå°\nåœ¨æœ€å‰é¢æ’å…¥ï¼Œç¡®ä¿é¡ºåºæ˜¯[R0, R1, R2...]\n1 2 3 4 5 6 7 8 9 def calculate_stepwise_returns(rewards, discount_factor): returns = [] R = 0 for r in reversed(rewards): # å…ˆ R = r + R * discount_factor returns.insert(0, R) returns = torch.tensor(returns) normalized_returns = (returns - returns.mean()) / returns.std() return normalized_returns ä¸ºä»€ä¹ˆè¦Normalize?\nå‡å°æ–¹å·® åŠ é€Ÿæ”¶æ•› 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def forward_pass(env, policy, discount_factor): log_prob_actions = [] rewards = [] done = False episode_return = 0 policy.train() observation, info = env.reset() while not done: observation = torch.FloatTensor(observation).unsqueeze(0) # state action_pred = policy(observation) # action = policy(state) action_prob = F.softmax(action_pred, dim = -1) dist = distributions.Categorical(action_prob) # discretize action = dist.sample() log_prob_action = dist.log_prob(action) observation, reward, terminated, truncated, info = env.step(action.item()) # reward = state(action) done = terminated or truncated log_prob_actions.append(log_prob_action) rewards.append(reward) episode_return += reward log_prob_actions = torch.cat(log_prob_actions) stepwise_returns = calculate_stepwise_returns(rewards, discount_factor) return episode_return, stepwise_returns, log_prob_actions è®¡ç®—loss\n1 2 3 def calculate_loss(stepwise_returns, log_prob_actions): loss = -(stepwise_returns * log_prob_actions).sum() return loss è¿™é‡Œçš„loss functionå°±æ˜¯æœ€åŸºæœ¬çš„policy gradient, REINFORCE\n$$ \\tt{Loss} = -\\sum_t (\\tt{Return}_t \\times \\log \\pi (a_t | s_t))$$$\\log \\pi (a_t | s_t)$ æ˜¯ç­–ç•¥ç½‘ç»œè¾“å‡ºçš„åŠ¨ä½œå¯¹æ•°æ¦‚ç‡ï¼ˆlog_prob_actionsï¼‰\nå¹¶åå‘ä¼ æ’­æ›´æ–°æƒé‡\n1 2 3 4 5 6 7 8 def update_policy(stepwise_returns, log_prob_actions, optimizer): stepwise_returns = stepwise_returns.detach() loss = calculate_loss(stepwise_returns, log_prob_actions) optimizer.zero_grad() loss.backward() optimizer.step() return loss.item() è®­ç»ƒå¹¶ä¿å­˜æƒé‡\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import gymnasium as gym import torch import numpy as np from torch import optim env = gym.make(\u0026#34;CartPole-v1\u0026#34;) seed = 66 np.random.seed(seed) torch.manual_seed(seed) env.reset(seed=seed) def main(): MAX_EPOCHS = 500 DISCOUNT_FACTOR = 0.99 N_TRIALS = 25 REWARD_THRESHOLD = 475 PRINT_INTERVAL = 10 INPUT_DIM = env.observation_space.shape[0] HIDDEN_DIM = 128 OUTPUT_DIM = env.action_space.n DROPOUT = 0.5 episode_returns = [] policy = PolicyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT) LEARNING_RATE = 0.01 optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE) for episode in range(1, MAX_EPOCHS+1): episode_return, stepwise_returns, log_prob_actions = forward_pass(env, policy, DISCOUNT_FACTOR) _ = update_policy(stepwise_returns, log_prob_actions, optimizer) episode_returns.append(episode_return) mean_episode_return = np.mean(episode_returns[-N_TRIALS:]) if episode % PRINT_INTERVAL == 0: print(f\u0026#39;| Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} |\u0026#39;) if mean_episode_return \u0026gt;= REWARD_THRESHOLD: print(f\u0026#39;Reached reward threshold in {episode} episodes\u0026#39;) torch.save(policy, \u0026#39;pg.pt\u0026#39;) break if __name__ == \u0026#34;__main__\u0026#34;: main() Load model and let it play\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import gymnasium as gym import torch env = gym.make(\u0026#34;CartPole-v1\u0026#34;, render_mode=\u0026#34;human\u0026#34;) observation, info = env.reset() policy = torch.load(\u0026#34;ppo.pt\u0026#34;, weights_only=False).eval() episode_over = False while not episode_over: obs_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0) # Get action from policy with torch.no_grad(): action_logits = policy(obs_tensor) action = torch.argmax(action_logits, dim=1).item() observation, reward, terminated, truncated, info = env.step(action) episode_over = terminated or truncated env.close() ","date":"2025-03-02T00:00:00Z","permalink":"https://gushroom.github.io/p/cartpole/","title":"Cartpole"},{"content":"the CLIP/SigLip model family has 3 parameters. VIT-Patch-Res. VIT is the vision transformer architecture and size. Patch is when they split the input image into smaller grids, what is the size of those smaller images, 16 means 16x16, 14 means 14x14. Res is the input image resolution. The higher the more accurate but bigger(slower).\nPrereq: install git-lfs https://github.com/git-lfs/git-lfs/blob/main/INSTALLING.md\nDownload model weight: git clone https:/huggingface.co/\u0026lt;repository\u0026gt;\nRunning the model using transformers 1 2 3 4 5 6 7 8 9 10 11 from transformers import AutoModel, AutoProcessor, AutoTokenizer # load the model and processor ckpt = \u0026#34;path/to/model/directory\u0026#34; model = AutoModel.from_pretrained(ckpt).to(device).eval() processor = AutoProcessor.from_pretrained(ckpt) tokenizer = AutoTokenizer.from_pretrained(ckpt) inputs = processor(images=[image], return_tensors=\u0026#34;pt\u0026#34;).to(model.device) with torch.no_grad(): image_embeddings = model.get_image_features(**inputs) Running Qwen-VL2.5 1 2 model = Qwen2_5_VLForConditionalGeneration.from_pretrained(ckpt, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;) processor = AutoProcessor.from_pretrained(ckpt) What\u0026rsquo;s not mentioned in the documentation, PIL.Image is also accepted as image input\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def describe_images(self, image: PIL.Image, prompt=\u0026#34;Describe this image\u0026#34;) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34;For large VLM that output natural language instead of vector embeddings\u0026#34;\u0026#34;\u0026#34; from qwen_vl_utils import process_vision_info if self._model is None: self.load_model() messages = [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;image\u0026#34;, \u0026#34;image\u0026#34;: image, }, {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: prompt}, ], } ] text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) image_inputs, video_inputs = process_vision_info(messages) inputs = self.processor( text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\u0026#34;pt\u0026#34;, ) image_grid_thw = inputs[\u0026#39;image_grid_thw\u0026#39;].numpy() input_h = image_grid_thw[0][1] * 14 input_w = image_grid_thw[0][2] * 14 inputs = inputs.to(self.device) generated_ids = self._model.generate(**inputs, max_new_tokens=128) generated_ids_trimmed = [ out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids) ] output_text = self.processor.batch_decode( generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False ) return output_text[0], (input_w, input_h) ","date":"2025-03-02T00:00:00Z","permalink":"https://gushroom.github.io/p/download-and-run-vlm-model-from-huggingface/","title":"Download and run VLM model from HuggingFace"},{"content":"Omniparser(https://github.com/microsoft/OmniParser) provides grounding for GUI elements. This allows for fully automation and controll over devices in an agentic way.\nclone the repo, and prepare the environment:\n1 2 3 4 cd OmniParser conda create -n \u0026#34;omni\u0026#34; python==3.12 conda activate omni pip install -r requirements.txt Model weights are here:\n1 2 git clone https://huggingface.co/microsoft/OmniParser git clone https://huggingface.co/microsoft/OmniParser-v2.0 BLIP and Florence models are in the Omniparser repository, while v-2.0 has a newer YOLO detection model.\nPut model weights under OmniParser/weights/\nAutoDL çš„ç³»ç»Ÿç›˜åªæœ‰30Gï¼Œæ‰€ä»¥æŠŠæ¨¡å‹æƒé‡æ”¾æ•°æ®ç›˜ï¼š\n1 2 3 4 5 6 ç»ˆç«¯ä¸­æ‰§è¡Œï¼š export HF_HOME=/root/autodl-tmp/cache/ æˆ–è€…Pythonä»£ç ä¸­æ‰§è¡Œï¼š import os os.environ[\u0026#39;HF_HOME\u0026#39;] = \u0026#39;/root/autodl-tmp/cache/\u0026#39; OCRçš„è¯­è¨€è®¾ç½®åœ¨util/utils.py\n1 2 3 4 5 6 7 8 9 10 reader = easyocr.Reader([\u0026#39;en\u0026#39;]) paddle_ocr = PaddleOCR( lang=\u0026#39;en\u0026#39;, # other lang also available use_angle_cls=False, use_gpu=False, # using cuda will conflict with pytorch in the same process show_log=False, max_batch_size=1024, use_dilation=True, # improves accuracy det_db_score_mode=\u0026#39;slow\u0026#39;, # improves accuracy rec_batch_num=1024) paddleçš„è¯ï¼ŒæŠŠenæ¢æˆchå°±æ˜¯ä¸­è‹±åŒè¯­çš„äº† easyå¯ä»¥è®¾ç½®æˆ[\u0026lsquo;ch_sim\u0026rsquo;,\u0026rsquo;en\u0026rsquo;]\næœ€åæŠŠç«¯å£è®¾ç½®è½¬å‘\n1 ssh -CNg -L 6006:127.0.0.1:6006 root@connect.nmb1.seetacloud.com -p 33674 åœ¨æœ¬åœ°å°±å¯ä»¥è¿è¡Œäº† ","date":"2025-03-01T00:00:00Z","permalink":"https://gushroom.github.io/p/running-omniparser-with-autodl-backend/","title":"Running Omniparser with AutoDL Backend"},{"content":"It all started with: Deep neural networks are universial function approximators.\nWhat is a unversial function approximator? A feed-forward artificial neural network with a single hidden layer and sigmoidal activation functions can pointwise approximate any continuous function of many variables with any predetermined accuracy.\nSuppose we have a set of points $(x, y)$ and assume that there is some regularity connecting $x$ and $y$:\n$$y=f(x)$$We can select and apply to $x$ such a weighting factor $w_{1}$ and such an offset $b_1$, that the sigmoid taken from them will pass through a part of our points or close enough to them:\n$$\\sigma_1=\\sigma(w_{1}x+b_1)$$We can then select and apply to $x $ such $w_{2}$ and $b_2$, that the sigmoid taken from them in sum with $\\sigma_ 1$ will pass still through a part of the points or close enough to them:\n$$\\sigma_2=\\sigma(w_{2}x+b_2)$$We can keep adding sigmoids and tinkering with their parameters until their sum approximates the pattern $f(x)$ expressed in the data accurately enough.\n$$\\sigma_1+\\sigma_2+...+\\sigma_n \\approx f(x)$$How do they become universial function approximators? As we all know, with just $y = (wx+b)$, we have linear regression. We know linear regressions are weak, they cannot solve XOR classification, as an example. What does the trick, is activation function. Activation functions introduce non-linearity to the model, allowing the model to fit to arbitrary functions.\nWhy we chose sigmoid()? Sigmoid() $\\sigma(x) = 1 - sigma(-x)$ function (0, 1) simulates the probability of \u0026ldquo;firing a neuron\u0026rdquo;, and it has been widely used in machine learning and statistics. It was one of the easiest functions to add non-linearity as well.\nFrom sigmoid() to tanh() Sigmoid() function only has positive values, it passes the same sign to update the weight matrices during back propagation, leading to inefficient gradient descent.\nTanh(), hyperbolic tangent, solves the above problem by centering at 0, so we can have both signs when updating weight. Converges faster.\nReLU and its brothers Vanishing gradient problem: As models get deeper, the gradient of the loss $\\mathcal{L}$ with respect to the weights in layer $l$ can be written as:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}_\\mathcal{L}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_L} \\times \\frac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{a}_{L-1}} \\times ... \\times \\frac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}}$$where $\\mathbf{a}_l$ is the activation at layer $l$.\nWe can see that when $\\partial \\mathbf{a}_l$ is less than 1, (especially sigmoid, the max is only 0.25), the result approaches 0 exponentially. Our model stops learning from here.\nReLU is a easy stepwise funtion that mitigates this problem. Its derivative is always 1 when input is positive, and it does not saturate.\nHowever, a rigid threshold of $y = 0(x \u0026lt;= 0)$ also causes problems.\nIt \u0026ldquo;turns off\u0026rdquo; neurons with nagative weight. Leaky ReLU It is not differentiable at $(x = 0)$ (Same holds for Leaky) GELU, Swish Normalization Optimizers Loss functions ","date":"2025-02-25T00:00:00Z","permalink":"https://gushroom.github.io/p/a-little-bit-on-deep-learning-in-general/","title":"A Little Bit on Deep Learning in General"},{"content":"CNN Convolution å·ç§¯ = ç‰¹å¾æå–\nç´¢ä¼¯ç®—å­ https://en.wikipedia.org/wiki/Sobel_operator æ—¢æ˜¯ä¼ ç»Ÿçš„å›¾åƒå¤„ç†ï¼Œä¹Ÿæ˜¯å¾ˆå¥½çš„ç‰¹å¾æå–ä¾‹å­ ä¸ºä»€ä¹ˆå·ç§¯çš„åŒæ—¶é€šé“æ•°ä¹Ÿåœ¨å¢åŠ ï¼Ÿ å·ç§¯å’Œæ± åŒ–éƒ½ä¼šå‡å°‘dimensionï¼Œå¢åŠ é€šé“æ•°æ¥ä¿è¯ä¸ä¸¢å¤±å¤ªå¤šä¿¡æ¯é‡ n_channelsä¸ªå·ç§¯æ ¸éšæœºåˆå§‹åŒ–ï¼Œå¯¹åŒä¸€ä¸ªfeature mapè¿›è¡Œç‰¹å¾æå– æ¯ä¸ªé€šé“å¯ä»¥å­¦ä¹ ä¸åŒçš„ç‰¹å¾ 1x1å·ç§¯æ ¸å¯ä»¥åœ¨ä¸æ”¹å˜feature mapå¤§å°çš„å‰æä¸‹æ”¹å˜é€šé“æ•° å‡ç»´/é™ç»´ é€šé“é—´ä¼ é€’ä¿¡æ¯ï¼šä½¿ç”¨1x1å·ç§¯æ ¸ï¼Œå®ç°é™ç»´å’Œå‡ç»´çš„æ“ä½œå…¶å®å°±æ˜¯channelé—´ä¿¡æ¯çš„çº¿æ€§ç»„åˆå˜åŒ–ï¼Œ3x3ï¼Œ64channelsçš„å·ç§¯æ ¸åé¢æ·»åŠ ä¸€ä¸ª1x1ï¼Œ28channelsçš„å·ç§¯æ ¸ï¼Œå°±å˜æˆäº†3x3ï¼Œ28channelsçš„å·ç§¯æ ¸ï¼ŒåŸæ¥çš„64ä¸ªchannelså°±å¯ä»¥ç†è§£ä¸ºè·¨é€šé“çº¿æ€§ç»„åˆå˜æˆäº†28channelsï¼Œè¿™å°±æ˜¯é€šé“é—´çš„ä¿¡æ¯äº¤äº’ Key components of CNN: Convolution: Extract feature from images, value in each cell are randomized and learned. Pooling: Reduce dimensionality to prevent overfitting, reduce computational cost(reduce size of feature map). Activation: Introduce Non-linearity. Fully connected layers: Perform classification. Size calculation: Convolution: $\\tt{size} = \\lfloor \\frac{(\\tt{width - kernelsize} + 2 \\times \\tt{padding})}{\\tt{stride}} \\rfloor + 1$ Pooling: $\\tt{size} = \\lceil \\frac{size}{2} \\rceil$ When size remains the same: kernelsize = 3 and padding = 1, kernel = 5 and padding = 2 VIT (Vision Transformer) Trains faster than CNN (transformer is more suitable for parallel computation) Requires slightly more dataset to train (at smaller datasets lose to CNN, excels at larger datasets) Attempt to apply transformer achitecture on image tasks without change represent image features like text patch(224 = 14px * 14px * (4x4)grid) Flattern pixels in the patch Linear projection of flatterned patches Add a learnable position encoder, attach to image token vectors ","date":"2025-02-25T00:00:00Z","permalink":"https://gushroom.github.io/p/computer-vision/","title":"Computer Vision"},{"content":"Answering all of my own questions about what I know that I dont know.\nWhat is encoder and decoder? Encoding Encoding is the process of extracting semantic information from languge. For some representation to be a good choice of encoding used in machine learning, it must satisfy 2 standards:\nIt must be easily digitized(understood by computers) The relationship between encodings must somehow represent the relationship between original language tokens. Let\u0026rsquo;s focus on (1.) first. To that end, we have tokenizer and one-hot encoding. Tokenizer gives each token an unique id, thus projecting the entire token space onto a one-dimensional axis(array). One-hot encoding uses unique binary ids, projecting each token onto it\u0026rsquo;s own dimension, keeping distances between encoded tokens the same. However they both fail to achieve the second standard.\nTokenizer makes it simple to calculate relationships(difference) between encoded tokens, but fails to capture the complex relationships between tokens, especially when some words have multiple meanings in different contexts. One-hot encoding cannot represent the relationship between tokens with relative distance at all. Token vectors are all perpendicular to each other, and the dot product is always 0. However, it makes addition simple, we can have combination of any vectors. Latent space and embedding We need some intermediate space, uses advantages from both tokenizer that captures relationship but is too dense, and one-hot encoding that allows easy combination(addition) of vectors but is too sparse. We can either add more dimensions to the one-dimensional tokenizer, or reduce dimensions from one-hot encoded space. Embedding is the process of reducing dimensionality from one-hot encoding by neural networks to a lower dimensional latent space.\nTokenizer - Splitting text corpuses into language primitives BPE (Byte-Pair Encoding) A very detailed step by step explanation can be found here: https://huggingface.co/learn/nlp-course/en/chapter6/5\nSplit text corpuses into primitives, such as individual characters(ascii, unicode), then iteratively merge the most frequent combination of the current primitives into a new token. We merge until we have got a satisfactory vocabulary size.\nBBPE Ascii and unicode have problems with representing unknown chars, such as Chinese, emoji. BBPE splits corpuses into bytes, having better representation and support for mulitlingual and special characters.\nHow do we poject token into embedding? We need to find a relationship to properly embed tokens as vectors.\nWord2Vec In 2013, Google proposed Word2Vec model, the first low-dimension word embedding, introducing two breakthroughs: Continuous Bag of Words(CBOW) and skip-gram.\nCBOW: collects words that appear before and after a target word in a sequence. For example, \u0026ldquo;The cat sits on the mat.\u0026rdquo; and the target word is \u0026ldquo;sits\u0026rdquo;, the model learns by trying to predict \u0026ldquo;sits\u0026rdquo; from [\u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;the\u0026rdquo;]. The model learns a weight to transform words into embeddings that has the max probability to predict the right word. Skip-grams: the opposite, predicts n context words based on a given target word. What it does: King - man + women $\\approx$ queen. Vector carries semantic meanings and can be \u0026ldquo;calculated\u0026rdquo;. Cosine similarity can actually find similarity between semantically close vectors. Problems with Word2Vec: It only considers one meaning of each word. (i.e. bank) Most importantly, it does not consider long range context information. LSTM Better aptures long range context, but still has a lot of issues:\nLSTM process word one by one, this cannot be parallelized, making training slow. Still struggles with longer context length. WIP\u0026hellip;Not tooo interested in RNN and LSTM and GRU tho\u0026hellip;\nTransformers Self Attention $$\\tt{Attention}(\\mathbf{Q, K, V}) = \\tt{softmax} \\lparen \\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\rparen \\mathbf{V}$$ An interesting observation is $\\mathbf{QK}^T$ is matrix multiplication, but it is called \u0026ldquo;dot product\u0026rdquo; because it effectively computes row-wise similarity score, and dot product is usually used for that purpose.\nThis similarity computation \u0026ldquo;assigns more weight\u0026rdquo; to any previous token $K$ that has a closer relationship with $Q$, allows the model to focus on more relevant information.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class SelfAttention(nn.Module): def __init__(self, dim_in, dim_out, qkv_bias = False): super().__init__() self.query = nn.Linear(dim_in, dim_out, bias=qkv_bias) self.key = nn.Linear(dim_in, dim_out, bias=qkv_bias) self.value = nn.Linear(dim_in, dim_out, bias=qkv_bias) def forward(self, x): Q = self.query(x) K = self.key(x) print(K.shape) V = self.value(x) attn_scores = Q @ K.T attn_weights = torch.softmax( attn_scores / K.shape[-1] ** 0.5, dim=-1 ) context_vec = attn_weights @ V return context_vec A Linear() layer computes $y = xW^T + b$, it tranforms input vector $x$ into another space by weight matrix $W$ and bias $b$. In this case, it projects $x$ onto the embedding space.\nThe softmax() function computes $\\tt{softmax}(x_i) = \\frac{e^{x_i}}{\\sum^n_{j=1}e^{x_j}}$. It projects points on the real number axis $x_i$ onto the function $y_i = e^{x_i}$. Then it considers the sum of all $y$ values to be 1. Now we have a non-zero probability score.\nAnother interesting fact about the softmax function is: if we multiply every $x$ with a factor, the relationship between $y$ will change. When this factor is less than 1, we will see the probability distribution moving towards a uniform distribution. This is the temperature parameter. The lower the temperature, the more \u0026ldquo;random\u0026rdquo; the results are.\nWhat\u0026rsquo;s wrong with self attention? It only has one QKV set. The model only learns one set of weight matrix, limiting its ability to capture more features of the data.\nMultihead Attention Proposed in the Attention is All You Need paper in 2017 with the transformers achitecture.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class MultiheadAttention(nn.Module): def __init__(self, embedding_size, num_heads): super().__init__() assert embedding_size % num_heads == 0, \u0026#34;Embedding size must be divisible by num_heads\u0026#34; self.embedding_size = embedding_size self.num_heads = num_heads self.head_dim = embedding_size // num_heads self.query = nn.Linear(embedding_size, embedding_size) self.key = nn.Linear(embedding_size, embedding_size) self.value = nn.Linear(embedding_size, embedding_size) self.out = nn.Linear(embedding_size, embedding_size) def forward(self, x): batch_size, sequence_length, embedding_size = x.shape Q = self.query(x) # shape: (batch_size, sequence_length, embedding_size) K = self.key(x) V = self.value(x) Q = Q.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) K = K.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) V = V.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) # Q,K,V.shape(batch_size, self.num_heads, sequence_length, self.head_dim) # Why view + transpose instead of view(batch_size, sequence_length, self.num_heads, self.head_dim)? # This is because how view() split tensor dimensions, we want to split embedding_size to num_heads * head_dim # But in the end we want (batch_size, self.num_heads, sequence_length, self.head_dim) for future calculations scores = torch.matmul(Q, K.transpose(-2, -1)) # K.T.shape: (batch_size, self.num_heads, self.head_dim, sequence_length) # Q @ K.T shape: (batch_size, self.num_heads, sequence_length, sequence_length) # (sequence_length, sequence_length) representing the attention scores for each head across all sequence positions normalized_scores = scores / self.head_dim ** 0.5 attention_weights = nn.functional.softmax(normalized_scores, dim = -1) attention_outputs = torch.matmul(attention_weights, V) # V.shape: (batch_size, self.num_heads, sequence_length, self.head_dim) # attention_output shape: (batch_size, self.num_heads, sequence_length, head_dim) combined_heads = attention_outputs.transpose(1, 2).contiguous().view(batch_size, sequence_length, embedding_size) # attention_outputs.transpose(1, 2): (batch_size, sequence_length, self.num_heads, head_dim) output = self.out(combined_heads) return output Note that besides the split and join of multiple attention heads, in the final layer we project attention output into embedding space.\nWhat\u0026rsquo;s wrong with MHA? MHA is great in terms of quality, the problem is computation cost. In MHA we repeat the attention calculation num_heads times, and that starts to become a problem when the model size get bigger. In order to compute attention_outputs, we need to load the QKV matricies into memory many times. and data transferring also becomes a bottleneck.\nSo we reduce amount of heads.\nMulti Query Attention Here we keep only one K and V, and we split Q into num_heads.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class MultiQueryAttention(nn.Module): def __init__(self, embedding_size, num_heads): super().__init__() assert embedding_size % num_heads == 0, \u0026#34;Embedding size must be divisible by num_heads\u0026#34; self.embedding_size = embedding_size self.num_heads = num_heads self.head_dim = embedding_size // num_heads # Separate query projection for multiple heads self.query = nn.Linear(embedding_size, embedding_size) # Shared key and value projection (single head) self.key = nn.Linear(embedding_size, self.head_dim) self.value = nn.Linear(embedding_size, self.head_dim) self.out = nn.Linear(embedding_size, embedding_size) def forward(self, x): batch_size, sequence_length, embedding_size = x.shape Q = self.query(x) # (batch_size, sequence_length, embedding_size) K = self.key(x) # (batch_size, sequence_length, head_dim) V = self.value(x) # (batch_size, sequence_length, head_dim) # Reshape queries to multiple heads Q = Q.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) # Keys and values are shared across all heads, so they have a single head dimension K = K.unsqueeze(1).expand(-1, self.num_heads, -1, -1) V = V.unsqueeze(1).expand(-1, self.num_heads, -1, -1) # K.unsqueeze(1) = (batch_size, 1, sequence_length, head_dim) # K.expand = (batch_size, num_heads, sequence_length, head_dim) scores = torch.matmul(Q, K.transpose(-2, -1)) / self.head_dim ** 0.5 # Q @ K.T = (batch_size, num_heads, sequence_length, sequence_length) attention_weights = nn.functional.softmax(scores, dim=-1) attention_outputs = torch.matmul(attention_weights, V) combined_heads = attention_outputs.transpose(1, 2).contiguous().view(batch_size, sequence_length, embedding_size) output = self.out(combined_heads) return output Group Query Attention Between MLA and MQA, uses G(1 \u0026laquo; G \u0026laquo; num_heads) Groups of KV pair, and still num_heads amount of Q matrices.\nWhen G = 1, GQA = MQA, when G = num_heads, GQA = MHA.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class GroupQueryAttention(nn.Module): def __init__(self, embedding_size, num_heads, num_groups): super().__init__() assert embedding_size % num_heads == 0, \u0026#34;embedding_size must be divisible by num_heads!\u0026#34; assert embedding_size % num_groups == 0, \u0026#34;embedding_size must be divisible by num_groups!\u0026#34; assert num_heads % num_groups == 0, \u0026#34;num_heads must be divisible by num_groups!\u0026#34; self.embedding_size = embedding_size self.num_heads = num_heads self.num_groups = num_groups self.head_dim = embedding_size // num_heads self.heads_per_group = num_heads // num_groups self.query = nn.Linear(embedding_size, embedding_size) self.key = nn.Linear(embedding_size, embedding_size // self.heads_per_group) self.value = nn.Linear(embedding_size, embedding_size // self.heads_per_group) self.out = nn.Linear(embedding_size, embedding_size) def forward(self, x): batch_size, sequence_length, embedding_size = x.shape Q = self.query(x) # (batch_size, sequence_length, embedding_size) K = self.key(x) # (batch_size, sequence_length, embedding_size // heads_per_group) V = self.value(x) # (batch_size, sequence_length, embedding_size // heads_per_group) Q = Q.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) K = K.view(batch_size, sequence_length, self.num_groups, -1).transpose(1, 2) V = V.view(batch_size, sequence_length, self.num_groups, -1).transpose(1, 2) # (batch_size, num_groups, sequence_length, head_dim) K = K.unsqueeze(2).expand(-1, -1, self.heads_per_group, -1, -1) V = V.unsqueeze(2).expand(-1, -1, self.heads_per_group, -1, -1) # expand heads_per_group into the third dimension # (batch_size, num_groups, heads_per_group, sequence_length, head_dim) K = K.contiguous().view(batch_size, self.num_heads, sequence_length, self.head_dim) ","date":"2025-02-25T00:00:00Z","permalink":"https://gushroom.github.io/p/overview-of-nlpllm/","title":"Overview of NLP(LLM)"},{"content":"Building a Chatbot with Flet ChatGPT has becoming more and more difficult to use with VPN. So I decided to build one with API. Getting famailar with Flet(flutter) is an extra because I want to build more stuff on this platform.\nFlet The interaction logic is very easy: onclick=function(). With buttons:\n1 2 3 4 5 6 7 8 9 send_button = ft.ElevatedButton( text=\u0026#34;Send\u0026#34;, on_click=send_message, width=page.width * 0.2, style=ft.ButtonStyle( color=ft.Colors.WHITE, bgcolor=ft.Colors.BLUE_400, ) ) To enable markdown in response, we need to replace the TextField with a Markdown element.\n1 2 3 4 5 6 7 output_box = ft.Markdown( value=\u0026#34;\u0026#34;, selectable=True, extension_set=ft.MarkdownExtensionSet.GITHUB_WEB, on_tap_link=lambda e: page.launch_url(e.data), width=page.width * 0.7, ) Saving chat history and memory to local storage: page.client_storage.set(key, value)\nLLM A basic template to use LLM APIs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class ChatBot(): def __init__(self, api_key, base_url=None, system_prompt=None, model=\u0026#34;o3-mini-high\u0026#34;): default_system_prompt = \u0026#34;You are a helpful assistant.\u0026#34; default_base_url = \u0026#34;https://api.openai-next.com/v1\u0026#34; self.system_prompt = system_prompt if system_prompt else default_system_prompt self.messages=[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: self.system_prompt}] self.memories=[] self.model = model self.base_url = base_url if base_url else default_base_url self.client = OpenAI(api_key=api_key, base_url=self.base_url) def chat(self, user_message): self.messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_message}) completion = self.client.chat.completions.create( model=self.model, stream=False, messages=self.messages ) response = completion.choices[0].message.content self.messages.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: response}) return response ","date":"2025-02-20T00:00:00Z","permalink":"https://gushroom.github.io/p/building-a-chatbot-with-flet/","title":"Building a Chatbot with Flet"},{"content":"Hugo theme Stack supports the creation of interactive image galleries using Markdown. It\u0026rsquo;s powered by PhotoSwipe and its syntax was inspired by Typlog.\nTo use this feature, the image must be in the same directory as the Markdown file, as it uses Hugo\u0026rsquo;s page bundle feature to read the dimensions of the image. External images are not supported.\nSyntax 1 ![Image 1](1.jpg) ![Image 2](2.jpg) Result Photo by mymind and Luke Chesser on Unsplash\n","date":"2025-02-20T00:00:00Z","image":"https://gushroom.github.io/p/image-gallery/2_hu_3e58a979f20e4e46.jpg","permalink":"https://gushroom.github.io/p/image-gallery/","title":"Image gallery"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;â€”\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\nâ€” Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] One line code block 1 \u0026lt;p\u0026gt;A paragraph\u0026lt;/p\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements â€” abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nreference other pages More on math typesetting\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-02-20T00:00:00Z","permalink":"https://gushroom.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Stack has built-in support for math typesetting using KaTeX.\nIt\u0026rsquo;s not enabled by default side-wide, but you can enable it for individual posts by adding math: true to the front matter. Or you can enable it side-wide by adding math = true to the params.article section in config.toml.\nInline math This is an inline mathematical expression: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887â€¦$\n1 $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887â€¦$ Block math $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ 1 2 3 $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ 1 2 3 $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ ","date":"2025-02-20T00:00:00Z","permalink":"https://gushroom.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"For more details, check out the documentation.\nBilibili video Tencent video YouTube video Generic video file Your browser doesn't support HTML5 video. Here is a link to the video instead. Gist GitLab Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nâ€• A famous person, The book they wrote Photo by Codioful on Unsplash\n","date":"2025-02-20T00:00:00Z","image":"https://gushroom.github.io/p/shortcodes/cover_hu_5667347daefb4230.jpg","permalink":"https://gushroom.github.io/p/shortcodes/","title":"Shortcodes"}]