<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Basics of DL(Activation, Loss, Optimizers)"><title>A Little Bit on Deep Learning in General</title>
<link rel=canonical href=https://gushroom.github.io/p/a-little-bit-on-deep-learning-in-general/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="A Little Bit on Deep Learning in General"><meta property='og:description' content="Basics of DL(Activation, Loss, Optimizers)"><meta property='og:url' content='https://gushroom.github.io/p/a-little-bit-on-deep-learning-in-general/'><meta property='og:site_name' content="Gushroom's notebook"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Deep Learning'><meta property='article:published_time' content='2025-02-25T00:00:00+00:00'><meta property='article:modified_time' content='2025-02-25T00:00:00+00:00'><meta name=twitter:title content="A Little Bit on Deep Learning in General"><meta name=twitter:description content="Basics of DL(Activation, Loss, Optimizers)"><link rel="shortcut icon" href=/favicon-32x32.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/self_hu_96cb9cc58f14df2b.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üçÑ</span></figure><div class=site-meta><h1 class=site-name><a href=/>Gushroom's notebook</a></h1><h2 class=site-description>ÊÖéÁªàÂ¶ÇÂßãÔºåÂàôÊó†Ë¥•‰∫ã„ÄÇ</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#what-is-a-unversial-function-approximator>What is a unversial function approximator?</a></li><li><a href=#how-do-they-become-universial-function-approximators>How do they become universial function approximators?</a><ol><li><a href=#why-we-chose-sigmoid>Why we chose sigmoid()?</a></li><li><a href=#from-sigmoid-to-tanh>From sigmoid() to tanh()</a></li><li><a href=#relu-and-its-brothers>ReLU and its brothers</a></li></ol></li><li><a href=#normalization>Normalization</a></li><li><a href=#optimizers>Optimizers</a></li><li><a href=#loss-functions>Loss functions</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/study-notes/ style=background-color:#90ee90;color:#fff>Study Notes</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/a-little-bit-on-deep-learning-in-general/>A Little Bit on Deep Learning in General</a></h2><h3 class=article-subtitle>Basics of DL(Activation, Loss, Optimizers)</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Feb 25, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>3 minute read</time></div></footer></div></header><section class=article-content><p>It all started with: <strong>Deep neural networks are universial function approximators.</strong></p><h2 id=what-is-a-unversial-function-approximator>What is a unversial function approximator?</h2><blockquote><p>A feed-forward artificial neural network with a single hidden layer and sigmoidal activation functions can pointwise approximate any continuous function of many variables with any predetermined accuracy.</p></blockquote><p>Suppose we have a set of points $(x, y)$ and assume that there is some regularity connecting $x$ and $y$:</p>$$y=f(x)$$<p>We can select and apply to $x$ such a weighting factor $w_{1}$ and such an offset $b_1$, that the sigmoid taken from them will pass through a part of our points or close enough to them:</p>$$\sigma_1=\sigma(w_{1}x+b_1)$$<p>We can then select and apply to $x $ such $w_{2}$ and $b_2$, that the sigmoid taken from them in sum with $\sigma_ 1$ will pass still through a part of the points or close enough to them:</p>$$\sigma_2=\sigma(w_{2}x+b_2)$$<p>We can keep adding sigmoids and tinkering with their parameters until their sum approximates the pattern $f(x)$ expressed in the data accurately enough.</p>$$\sigma_1+\sigma_2+...+\sigma_n \approx f(x)$$<h2 id=how-do-they-become-universial-function-approximators>How do they become universial function approximators?</h2><p>As we all know, with just $y = (wx+b)$, we have linear regression. We know linear regressions are weak, they cannot solve XOR classification, as an example. What does the trick, is activation function. Activation functions introduce non-linearity to the model, allowing the model to fit to arbitrary functions.</p><h3 id=why-we-chose-sigmoid>Why we chose sigmoid()?</h3><p>Sigmoid() $\sigma(x) = 1 - sigma(-x)$ function (0, 1) simulates the probability of &ldquo;firing a neuron&rdquo;, and it has been widely used in machine learning and statistics. It was one of the easiest functions to add non-linearity as well.</p><h3 id=from-sigmoid-to-tanh>From sigmoid() to tanh()</h3><p>Sigmoid() function only has positive values, it passes the same sign to update the weight matrices during back propagation, leading to inefficient gradient descent.</p><p>Tanh(), hyperbolic tangent, solves the above problem by centering at 0, so we can have both signs when updating weight. Converges faster.</p><h3 id=relu-and-its-brothers>ReLU and its brothers</h3><p><strong>Vanishing gradient problem</strong>: As models get deeper, the gradient of the loss $\mathcal{L}$ with respect to the weights in layer $l$ can be written as:</p>$$ \frac{\partial \mathcal{L}}{\partial \mathbf{w}_\mathcal{L}} = \frac{\partial \mathcal{L}}{\partial \mathbf{a}_L} \times \frac{\partial \mathbf{a}_L}{\partial \mathbf{a}_{L-1}} \times ... \times \frac{\partial \mathbf{a}_{l+1}}{\partial \mathbf{a}_{l}}$$<p>where $\mathbf{a}_l$ is the activation at layer $l$.</p><p>We can see that when $\partial \mathbf{a}_l$ is less than 1, (especially sigmoid, the max is only 0.25), the result approaches 0 exponentially. Our model stops learning from here.</p><p><img src=/p/a-little-bit-on-deep-learning-in-general/d_sigmoid.png width=999 height=252 srcset="/p/a-little-bit-on-deep-learning-in-general/d_sigmoid_hu_50a03c0afe755742.png 480w, /p/a-little-bit-on-deep-learning-in-general/d_sigmoid_hu_72a395e2b64408b1.png 1024w" loading=lazy alt="Derivative of sigmoid function" class=gallery-image data-flex-grow=396 data-flex-basis=951px></p><p><strong>ReLU</strong> is a easy stepwise funtion that mitigates this problem. Its derivative is always 1 when input is positive, and it does not saturate.</p><p>However, a rigid threshold of $y = 0(x &lt;= 0)$ also causes problems.</p><ol><li>It &ldquo;turns off&rdquo; neurons with nagative weight.<ul><li>Leaky ReLU</li></ul></li><li>It is not differentiable at $(x = 0)$ (Same holds for Leaky)<ul><li>GELU, Swish</li></ul></li></ol><p><img src=/p/a-little-bit-on-deep-learning-in-general/ReLUs.jpg width=553 height=301 srcset="/p/a-little-bit-on-deep-learning-in-general/ReLUs_hu_b3ca591aa44224ce.jpg 480w, /p/a-little-bit-on-deep-learning-in-general/ReLUs_hu_5d4de1423e26a5b1.jpg 1024w" loading=lazy alt="ReLU and its modifications" class=gallery-image data-flex-grow=183 data-flex-basis=440px></p><h2 id=normalization>Normalization</h2><h2 id=optimizers>Optimizers</h2><h2 id=loss-functions>Loss functions</h2></section><footer class=article-footer><section class=article-tags><a href=/tags/deep-learning/>Deep Learning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/cartpole/><div class=article-details><h2 class=article-title>Cartpole</h2></div></a></article><article><a href=/p/download-and-run-vlm-model-from-huggingface/><div class=article-details><h2 class=article-title>Download and run VLM model from HuggingFace</h2></div></a></article><article><a href=/p/computer-vision/><div class=article-details><h2 class=article-title>Computer Vision</h2></div></a></article><article><a href=/p/overview-of-nlpllm/><div class=article-details><h2 class=article-title>Overview of NLP(LLM)</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2025 Gushroom's notebook</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>